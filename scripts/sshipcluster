#!/usr/bin/env python
"""
Start an IPython.parallel cluster inside a PBS script, using ssh to spawn
ipengine instances.

Example PBS Script
------------------
#PBS -l nodes=2:ppn=16

sshipcluster --daemonize   # start the cluster. 32 workers will boot

# Run your python scripts that connects to the workers and runs jobs
python -c 'import IPython.parallel; ...'
-------------------
"""
from __future__ import print_function, absolute_import, division
import argparse
from IPython.utils.daemonize import daemonize
import multiprocessing
import os
import socket
import tempfile
import threading
import time

parser = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument('--unique',
                    help='Start only ipengine per node (use this if your ' +
                         'processes are individually node-parallel).',
                    action='store_true', default=False)
parser.add_argument('-v', '--verbose', help='Verbose mode (default: True)',
                    action='store_true', default=True)
parser.add_argument('--daemonize', help='Daemonize (default: False)',
                    action='store_true', default=False)
parser.add_argument('-id', '--cluster-id',
                    help='Set the cluster ID to avoid collisions.')
args = parser.parse_args()


def log(msg):
    if args.verbose:
        print(msg)


def main():
    if 'PBS_NODEFILE' in os.environ:
        if args.unique:
            fid, nodefile = tempfile.mkstemp()
            os.close(fid)
            log('Getting unique entries in PBS_NODEFILE')
            with open(os.environ['PBS_NODEFILE']) as f:
                nodes = set(f.readlines())

            log('{:d} unique nodes')
            with open(nodefile, 'w') as f:
                for node in nodes:
                    f.write(node)
        else:
            nodefile = os.environ['PBS_NODEFILE']
    else:
        log('Not running under PBS')
        fid, nodefile = tempfile.mkstemp()
        os.close(fid)
        count = 1 if args.unique else multiprocessing.cpu_count()
        with open(nodefile, 'w') as f:
            for _ in range(count):
                f.write('%s\n' % socket.gethostname())

    # parse nodefile
    hosts = []
    with open(nodefile) as f:
        for line in f:
            host = line.strip()
            hosts.append(host)

    q = '' if args.verbose else '--quiet'
    if 'PBS_O_WORKDIR' in os.environ:
        work_dir = '--work-dir={}'.format(os.environ['PBS_O_WORKDIR'])
    else:
        work_dir = ''
    if args.cluster_id:
        cluster_id = '--cluster-id={}'.format(args.cluster_id)
    else:
        cluster_id = ''

    # start controller
    controller = threading.Thread(target=lambda: os.system(
        'ipcontroller --ip=* {} {} {}'.format(work_dir, cluster_id, q)))
    controller.daemon = True
    controller.start()
    time.sleep(1)  # wait for the controller to load

    # start engines
    for host in hosts:
        engine = threading.Thread(target=lambda: os.system(
            'ssh {} ipengine {} {} {}'.format(host, work_dir, cluster_id, q)))
        engine.daemon = True
        engine.start()

    # wait for engines to load
    time.sleep(10)

    if args.daemonize:
        daemonize()

    while True:
        time.sleep(1)

if __name__ == '__main__':
    main()
